{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "def open_file(year,language,OG,analyzed):\n",
    "    # script_dir = os.path.abspath(__file__)  \n",
    "    # data_dir = os.path.join(script, \"Data\")\n",
    "    # print(data_dir)\n",
    "    # data_dir = os.path.join(os.path.dirname(__file__), \"Data\")\n",
    "    data_dir = 'C:/Users/theoh/Documents/FootdSafetyEngagement/Data'\n",
    "    filename = f\"{data_dir}/tweets_data_{language}_{year}\"\n",
    "    if OG:\n",
    "        filename += \"_VO\" \n",
    "    if OG == False:\n",
    "        filename += \"_VE\"\n",
    "    if analyzed:\n",
    "        filename += \"_sentiment\"\n",
    "    try:\n",
    "        with open(filename+'.csv', 'r') as file:\n",
    "            return pd.read_csv(file,sep=',')\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "    \n",
    "def get_file_name(year,language,OG):\n",
    "    data_dir = 'C:/Users/theoh/Documents/FootdSafetyEngagement/Data'\n",
    "    filename = f\"{data_dir}/tweets_data_{language}_{year}\"\n",
    "    if OG:\n",
    "        filename += \"_VO\" \n",
    "    if OG == False:\n",
    "        filename += \"_VE\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theoh\\miniconda3\\envs\\Stage\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "\n",
    "def sentiment_analyzing(year,language):\n",
    "    if language == 'en':\n",
    "        tweets_data = open_file(year, language, None,False)\n",
    "    else:\n",
    "        tweets_data = pd.concat([open_file(year, language, True,False),open_file(year, language, False,False)],ignore_index=True)   \n",
    "\n",
    "    # Charger le modèle et le tokenizer multilingue\n",
    "    model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Fonction pour analyser le sentiment d'un tweet\n",
    "    def analyze_sentiment(tweet):\n",
    "        inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        scores = outputs[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        positive_score = scores[2]  # Utiliser l'indice pour le score positif selon le modèle multilingue\n",
    "        return 1 if positive_score > 0.5 else 0\n",
    "\n",
    "    # Appliquer l'analyse de sentiment à chaque tweet\n",
    "    tweets_data['sentiment'] = tweets_data['content'].apply(analyze_sentiment)\n",
    "\n",
    "    # Sauvegarder les résultats dans un fichier CSV\n",
    "    def save_to_csv(dataframe, file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            dataframe.to_csv(file_path, mode='a', index=False, header=False, encoding='utf-8')\n",
    "        else:\n",
    "            dataframe.to_csv(file_path, mode='w', index=False, encoding='utf-8')\n",
    "\n",
    "    save_to_csv(tweets_data, get_file_name(year,language,None) + '_sentiment.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "333    0\n",
      "334    0\n",
      "335    0\n",
      "336    0\n",
      "337    0\n",
      "Name: sentiment, Length: 338, dtype: int64\n",
      "Sur les tweets en anglais,  16 sont positifs et  322 sont négatifs.\n"
     ]
    }
   ],
   "source": [
    "# sentiment_analyzing(2024,'en')\n",
    "tweets_data_en_analyzed = open_file(2024,'en',None,True)\n",
    "print(tweets_data_en_analyzed['sentiment'])\n",
    "\n",
    "print(\"Sur les tweets en anglais, \",tweets_data_en_analyzed['sentiment'].sum(),\"sont positifs et \",len(tweets_data_en_analyzed)-tweets_data_en_analyzed['sentiment'].sum(),\"sont négatifs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
